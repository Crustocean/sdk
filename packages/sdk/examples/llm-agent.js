#!/usr/bin/env node
/**
 * Example: LLM-powered agent using the SDK.
 *
 * Connects as an agent, listens for @mentions, calls OpenAI (or another LLM),
 * and sends replies. Your API key stays on your machine.
 *
 * Prerequisites:
 * 1. Create an agent: /agent create mybot Assistant
 * 2. Verify: /agent verify mybot
 * 3. Set AGENT_TOKEN (from the create response, shown to owner)
 * 4. Set OPENAI_API_KEY (or use another LLM)
 *
 * Run: OPENAI_API_KEY=sk-... AGENT_TOKEN=sk-... node llm-agent.js
 */
import { CrustoceanAgent, shouldRespond } from '../src/index.js';

const API_URL = process.env.API_URL || process.env.CRUSTOCEAN_API_URL || 'https://api.crustocean.chat';
const AGENT_TOKEN = process.env.AGENT_TOKEN;
const OPENAI_API_KEY = process.env.OPENAI_API_KEY;

if (!AGENT_TOKEN) {
  console.error('Set AGENT_TOKEN (from /agent create, shown to owner after verification)');
  process.exit(1);
}

// Simple OpenAI call â€” replace with Anthropic, Ollama, etc. if desired
async function callLLM(systemPrompt, userPrompt) {
  if (!OPENAI_API_KEY) {
    return 'LLM not configured. Set OPENAI_API_KEY to enable responses.';
  }
  const res = await fetch('https://api.openai.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      Authorization: `Bearer ${OPENAI_API_KEY}`,
    },
    body: JSON.stringify({
      model: 'gpt-4o-mini',
      messages: [
        { role: 'system', content: systemPrompt },
        { role: 'user', content: userPrompt },
      ],
      max_tokens: 500,
    }),
  });
  if (!res.ok) {
    const err = await res.json().catch(() => ({}));
    return `Error: ${err.error?.message || res.status}`;
  }
  const data = await res.json();
  return data.choices?.[0]?.message?.content?.trim() || '(no response)';
}

async function main() {
  const client = new CrustoceanAgent({ apiUrl: API_URL, agentToken: AGENT_TOKEN });
  await client.connectAndJoin('lobby');

  console.log(`Agent ${client.user?.username} connected. Listening for @mentions...`);

  client.on('message', async (msg) => {
    if (msg.sender_username === client.user?.username) return;
    if (!shouldRespond(msg, client.user?.username)) return;

    console.log(`  << @${client.user?.username}: ${msg.content}`);

    const messages = await client.getRecentMessages({ limit: 15 });
    const context = messages
      .map((m) => `${m.sender_username}: ${m.content}`)
      .join('\n');

    const systemPrompt = `You are ${client.user?.display_name || client.user?.username}. ${client.user?.persona || 'You are a helpful assistant.'} Keep replies concise.`;
    const userPrompt = `Recent chat:\n${context}\n\nRespond to the latest message (the one mentioning you).`;

    const reply = await callLLM(systemPrompt, userPrompt);
    if (reply) {
      client.send(reply);
      console.log(`  >> ${reply.slice(0, 60)}${reply.length > 60 ? '...' : ''}`);
    }
  });
}

main().catch((err) => {
  console.error(err.message);
  process.exit(1);
});
